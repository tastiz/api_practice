{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSaZGSLfQ7cBqHQmhqMtSq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tastiz/api_practice/blob/master/intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSV6gS3jZaq3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "bf66f90d-2c1a-4d35-963e-de7077c1b928"
      },
      "source": [
        "import requests\n",
        "\n",
        "query = {'q': 'tetris+language:assembly', 'sort': 'stars', 'order': 'desc'}\n",
        "url = 'https://api.github.com/search/repositories'\n",
        "response = requests.get(url, params=query)\n",
        "response.status_code\n",
        "response.ok\n",
        "data = response.json()\n",
        "data\n",
        "# List all the keys in the dictionary\n",
        "print(data.keys())\n",
        "\n",
        "# What is the value of total_count?\n",
        "print(f'Total number of repositories matching query is {data[\"total_count\"]}')\n",
        "\n",
        "# how many items in the items array\n",
        "print(f'The length of the items array is {len(data[\"items\"])}')\n",
        "\n",
        "# get the first item from the array\n",
        "item1 = data['items'][0]\n",
        "\n",
        "# What are the keys of the item?\n",
        "print(item1.keys())\n",
        "\n",
        "broken_url = 'http://api.github.com/this_is_broken'\n",
        "broken_response = requests.get(broken_url)\n",
        "print(broken_response.ok)\n",
        "print(broken_response.status_code)\n",
        "\n",
        "some_data = broken_response.json()\n",
        "some_data\n",
        "\n",
        "import requests\n",
        "\n",
        "url = 'https://api.fda.gov/food/enforcement.json?search=report_date:[20180101+TO+20181231]'\n",
        "query = {'limit': 99}\n",
        "response = requests.get(url, query)\n",
        "response.ok\n",
        "\n",
        "# make sure we got a valid response\n",
        "if(response.ok):\n",
        "  # get the full data from the response\n",
        "  data = response.json()\n",
        "  # get just the results\n",
        "  raw_incidents = data['results']\n",
        "  #iterate the results and only grab the properties that we are interetsed in\n",
        "  incidents = [{\n",
        "      'city':incident['city'],\n",
        "      'state': incident['state'],\n",
        "      'reason': incident['reason_for_recall'],\n",
        "      'date': incident['report_date'],\n",
        "      'company': incident['recalling_firm'],\n",
        "      'product_type': incident['product_type'],\n",
        "      'postal_code': incident['postal_code']\n",
        "  } for incident in raw_incidents]\n",
        "  # print 5 items to see if this worked\n",
        "  print(incidents[:5])\n",
        "\n",
        "  # define a function that will process the data for us\n",
        "def process_data(raw_incidents):\n",
        "  #iterate the results and only grab the properties that we are interetsed in\n",
        "  return [{\n",
        "      'city':incident['city'],\n",
        "      'state': incident['state'],\n",
        "      'reason': incident['reason_for_recall'],\n",
        "      'date': incident['report_date'],\n",
        "      'company': incident['recalling_firm'],\n",
        "      'product_type': incident['product_type'],\n",
        "      'postal_code': incident['postal_code']\n",
        "  } for incident in raw_incidents]\n",
        "  \n",
        "\n",
        "# declare a list to store all results\n",
        "incidents = []\n",
        "\n",
        "# declare variables to track skip amount\n",
        "skip = 0\n",
        "limit = 99\n",
        "\n",
        "# make an initial call\n",
        "url = 'https://api.fda.gov/food/enforcement.json?search=report_date:[20180101+TO+20181231]'\n",
        "query = {'limit': limit, 'skip': skip}\n",
        "response = requests.get(url, query)\n",
        "print('Querying {}'.format(response.url))\n",
        "\n",
        "# make sure we got a valid response\n",
        "if(response.ok):\n",
        "  # get the full data from the response\n",
        "  data = response.json()\n",
        "  \n",
        "  # get the meta data\n",
        "  meta_data = data['meta']\n",
        "\n",
        "  total = meta_data['results']['total']\n",
        "  print('There is a total of {} results to fetch'.format(total))\n",
        "  \n",
        "  # process the results we have so far\n",
        "  incidents = process_data(data['results'])\n",
        "  print('{} results processed so far'.format(len(incidents)))\n",
        "  \n",
        "  # increment skip\n",
        "\n",
        "  skip = skip + limit\n",
        "  \n",
        "  while skip < total:\n",
        "    query = {'limit': limit, 'skip': skip}\n",
        "    response = requests.get(url, query)\n",
        "    print('Querying {}'.format(response.url))\n",
        "    if(response.ok):\n",
        "      #  now incidents will be the old values plus the new ones      \n",
        "      incidents = incidents + process_data(response.json()['results'])\n",
        "      print('{} results processed so far'.format(len(incidents)))\n",
        "      # increment skip\n",
        "      skip = skip + limit\n",
        "      \n",
        "print('{} results returned'.format(len(incidents)))   \n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['total_count', 'incomplete_results', 'items'])\n",
            "Total number of repositories matching query is 277\n",
            "The length of the items array is 30\n",
            "dict_keys(['id', 'node_id', 'name', 'full_name', 'private', 'owner', 'html_url', 'description', 'fork', 'url', 'forks_url', 'keys_url', 'collaborators_url', 'teams_url', 'hooks_url', 'issue_events_url', 'events_url', 'assignees_url', 'branches_url', 'tags_url', 'blobs_url', 'git_tags_url', 'git_refs_url', 'trees_url', 'statuses_url', 'languages_url', 'stargazers_url', 'contributors_url', 'subscribers_url', 'subscription_url', 'commits_url', 'git_commits_url', 'comments_url', 'issue_comment_url', 'contents_url', 'compare_url', 'merges_url', 'archive_url', 'downloads_url', 'issues_url', 'pulls_url', 'milestones_url', 'notifications_url', 'labels_url', 'releases_url', 'deployments_url', 'created_at', 'updated_at', 'pushed_at', 'git_url', 'ssh_url', 'clone_url', 'svn_url', 'homepage', 'size', 'stargazers_count', 'watchers_count', 'language', 'has_issues', 'has_projects', 'has_downloads', 'has_wiki', 'has_pages', 'forks_count', 'mirror_url', 'archived', 'disabled', 'open_issues_count', 'license', 'forks', 'open_issues', 'watchers', 'default_branch', 'score'])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}